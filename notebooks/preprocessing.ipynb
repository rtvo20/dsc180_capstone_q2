{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, remove\n",
    "from os.path import isfile, join\n",
    "from json import load, loads\n",
    "from tifffile import imread,imwrite\n",
    "from re import search, findall\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in Characterization and PSK Worklists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test cell, reads in process and characterization worklists\n",
    "f = open('maestro_logs/char_logs/maestro_sample_log_char.json')\n",
    "char_data = load(f)\n",
    "f = f.close()\n",
    "\n",
    "f = open('maestro_logs/step_logs/maestro_sample_log_process.json')\n",
    "process_data = load(f)\n",
    "f = f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `action.csv` file\n",
    "## Helper functions for `action.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def step_helper(worklists):\n",
    "    \"\"\"\n",
    "    Helper function to get the step names for each step in worklist(s).\n",
    "    Takes either a single worklist or a list of worklists in as input and returns\n",
    "    a list of lists, with each list corresponding to an input worklist.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "        \n",
    "    # iterate through worklists and get steps e.g. destination, drops, etc\n",
    "    for w in worklists:\n",
    "        w_steps = []\n",
    "        for step in w:\n",
    "            w_steps.append(list(step['details'].keys())[0])\n",
    "        steps.append(w_steps)\n",
    "    \n",
    "    return steps\n",
    "\n",
    "def dissolve_helper(drop_list, step_num = None):\n",
    "    drop_steps = drop_list['details']['drops']\n",
    "    \n",
    "    num_chems = len(drop_steps[0]['solution']['solutes'].split('_')) + \\\n",
    "    len(drop_steps[0]['solution']['solvent'].split('_'))\n",
    "    \n",
    "    actions = []\n",
    "    for i in range(1, num_chems+1):\n",
    "        row = {}\n",
    "        row['step_id'] = i\n",
    "        row['action'] = 'dissolve'\n",
    "        row['chemical_from'] = i\n",
    "        actions.append(row)\n",
    "    return actions    \n",
    "\n",
    "def drop_helper(drop_list, step_num):\n",
    "    drop_list = drop_list['details']['drops']\n",
    "    \n",
    "    # exclude volume and solution information, as this is handled when \n",
    "    # creating the chem csvs/nodes\n",
    "    exclude_attributes = ['volume', 'solution']\n",
    "    row_attributes = [i for i in drop_list[0].keys() if i not in exclude_attributes]\n",
    "    drop_rows = []\n",
    "    i = 1\n",
    "    for drop in drop_list:\n",
    "        row = {}\n",
    "        for j in row_attributes:\n",
    "            row['step_id'] = step_num * 2 + i\n",
    "            row['action'] = 'drop'\n",
    "            row['chemical_from'] = step_num + i\n",
    "            row[\"drop_\"+j] = drop.get(j)\n",
    "        i += 1\n",
    "        drop_rows.append(row)\n",
    "    return drop_rows\n",
    "\n",
    "def spin_helper(spin_list, step_num):\n",
    "    spin_rows = []\n",
    "    spin_details = spin_list['details']['steps']\n",
    "    for spin in spin_details:\n",
    "        row = {}\n",
    "        row['step_id'] = step_num\n",
    "        row['action'] = 'spin'\n",
    "        for i in spin:\n",
    "            row[\"spin_\"+i] = spin.get(i)\n",
    "        step_num += 2\n",
    "        \n",
    "        attributes = ['start', 'start_actual', 'finish_actual', 'liquidhandler_timings', 'spincoater_log']\n",
    "        for i in attributes:\n",
    "            if i in spin_list.keys():\n",
    "                if (i == 'spincoater_log') and ('rpm' in spin_list[i]):\n",
    "                    log_attr = spin_list[i].keys()\n",
    "                    for j in log_attr:\n",
    "                        row['spin_log_'+j] = spin_list[i][j]\n",
    "                else:\n",
    "                    row[i] = spin_list[i]\n",
    "        spin_rows.append(row)\n",
    "    return spin_rows\n",
    "\n",
    "def anneal_helper(anneal_list, step_num):\n",
    "    row = []\n",
    "    anneal_info = anneal_list['details']\n",
    "    anneal_row = {'step_id':step_num, 'action': 'anneal'}\n",
    "    for i in anneal_info:\n",
    "        anneal_row['anneal_'+i] = anneal_info.get(i)\n",
    "        \n",
    "    attributes = [i for i in anneal_list.keys() if i not in ['precedent', 'id', 'details']]\n",
    "    for i in attributes:\n",
    "        anneal_row[i] = anneal_list[i]\n",
    "    row.append(anneal_row)\n",
    "    return row\n",
    "\n",
    "def rest_helper(rest_step, step_num):\n",
    "    rest_row = {'step_id': step_num, 'action':'rest'}\n",
    "    rest_row['rest_duration'] = rest_step['details']['duration']\n",
    "    \n",
    "    attributes = [i for i in rest_step.keys() if i not in ['precedent', 'id', 'details']]\n",
    "    for i in attributes:\n",
    "        rest_row[i] = rest_step[i]\n",
    "    return [rest_row]\n",
    "\n",
    "def char_helper(char_list, step_num):\n",
    "    char_rows = []\n",
    "    char_details = char_list['details']['characterization_tasks']\n",
    "    for char in char_details:\n",
    "        char_params = [i for i in list(char['details'].keys())]\n",
    "        char_info = [i for i in char if i != 'details']\n",
    "        row = {\n",
    "            'step_id':step_num, \n",
    "            'action':'char'\n",
    "        }\n",
    "        for i in char_params:\n",
    "            row['char_'+i] = char['details'][i]\n",
    "        for i in char_info:\n",
    "            if type(char[i]) == str:\n",
    "                if i == 'name':\n",
    "                    row['char_'+i] = char[i].lower().split(\"_\")[0]\n",
    "                else:\n",
    "                    row['char_'+i] = char[i].lower()\n",
    "            else:\n",
    "                row['char_'+i] = char[i]\n",
    "                \n",
    "        attributes = [i for i in char_list.keys() if i not in ['precedent', 'id', 'details', 'name', 'sample', ]]\n",
    "        for i in attributes:\n",
    "            row[i] = char_list[i]\n",
    "        char_rows.append(row)\n",
    "        step_num += 2\n",
    "        \n",
    "    return char_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE: function map used in action_table to call helper functions\n",
    "func_map = {\n",
    "    \"dissolve\": dissolve_helper,\n",
    "    \"drops\": drop_helper,\n",
    "    \"spin\": spin_helper,\n",
    "    \"anneal\": anneal_helper,\n",
    "    \"duration\": rest_helper,\n",
    "    \"characterization_tasks\": char_helper\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing `action.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def action_table(worklists, sample_id=np.nan, batch_id=np.nan):\n",
    "    \"\"\"\n",
    "    The action_table function takes in one or more worklists. \n",
    "    If more than one worklist, include it in a list.\n",
    "    \"\"\"\n",
    "    if type(worklists[0]) != list:\n",
    "        worklists = [worklists]\n",
    "        \n",
    "    # obtain steps from worklists\n",
    "    steps = step_helper(worklists)\n",
    "    \n",
    "    rows = []\n",
    "    step_num = 1\n",
    "    for i in range(len(worklists)):\n",
    "        curr_worklist = worklists[i]\n",
    "        curr_steplist = steps[i]\n",
    "        for j in range(len(curr_steplist)):\n",
    "            if curr_steplist[j] == 'destination':\n",
    "                continue\n",
    "            else:\n",
    "                if curr_steplist[j] == 'drops':\n",
    "                    rows = rows + func_map['dissolve'](curr_worklist[j], step_num)\n",
    "                    step_num = rows[-1]['step_id']\n",
    "                    rows = rows + func_map['drops'](curr_worklist[j], step_num)\n",
    "                    step_num = rows[-1]['step_id']+3\n",
    "                    rows = rows + func_map['spin'](curr_worklist[j], step_num)\n",
    "                    step_num = rows[-1]['step_id']+2\n",
    "                else:\n",
    "                    if curr_steplist[j] == 'duration' and curr_worklist[j]['name'] == 'anneal':\n",
    "                        rows = rows + func_map['anneal'](curr_worklist[j], step_num)\n",
    "                    else:\n",
    "                        rows = rows + func_map[curr_steplist[j]](curr_worklist[j], step_num)\n",
    "                    step_num = rows[-1]['step_id']+2\n",
    "    \n",
    "    res = pd.DataFrame(rows)\n",
    "    res['sample_id'] = [sample_id] * res.shape[0]\n",
    "    res['batch_id'] = [batch_id] * res.shape[0]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterization Task\n",
    "Need this as an input for the `link.csv` file.\n",
    "## Formatting `.tif` images to a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "# helper function used in char_outputs.\n",
    "def load_image(fid):\n",
    "    img = imread(fid) * 64 * 255\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140]) #convert to single channel/greyscale??\n",
    "    return img.astype(int) #truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def char_outputs(folder, sample):\n",
    "    path = folder + '/' + sample + \"/characterization0\"\n",
    "    fids = [f for f in listdir(path)]\n",
    "    \n",
    "    data = []\n",
    "    ids = []\n",
    "    for fid in fids:\n",
    "        if '.tif' in fid:\n",
    "            data.append(load_image(path+\"/\"+fid))\n",
    "        elif '.csv' in fid:\n",
    "            data.append(pd.read_csv(path+\"/\"+fid).drop(0,axis=0).to_dict())\n",
    "        else:\n",
    "            print('haven\\'t had to deal w this filetype yet')\n",
    "\n",
    "        ids.append(fid.split('_', 1)[1].split('.')[0])    \n",
    "    df = pd.DataFrame({'join_on': ids, 'fid':fids, 'output':data})\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def append_outputs(output_df, action_df):\n",
    "    step_id = action_df.iloc[-1]['step_id']+2\n",
    "    \n",
    "    row_template = action_df.iloc[-1].copy()\n",
    "    row_template.loc[:]=np.nan\n",
    "    row_template['action'] = 'char'\n",
    "    row_template['sample_id'] = action_df['sample_id'].iloc[0]\n",
    "    row_template['batch_id'] = action_df['batch_id'].iloc[0]\n",
    "    \n",
    "    output_rows = []\n",
    "    for r in range(output_df.shape[0]):\n",
    "        output_row = output_df.iloc[r]\n",
    "        row = row_template.copy()\n",
    "        row['step_id'] = step_id\n",
    "        row['char_name'] = output_row['join_on']\n",
    "        row['fid'] = output_row['fid']\n",
    "        row['output'] = output_row['output']\n",
    "        output_rows.append(row)\n",
    "        step_id += 1\n",
    "        \n",
    "    action_df = action_df.append(output_rows)\n",
    "    \n",
    "    return action_df   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the `action.csv` file\n",
    "#### 'batch_id' and 'folder' here can be changed to the necessary batch # + folder where characterization outputs are stored.\n",
    "* currently the batch id and folder corresponds to our test batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "# run to save all as csvs\n",
    "action_dfs = []\n",
    "batch_id = 'b19'\n",
    "folder = 'Characterization_B19'\n",
    "\n",
    "for s in process_data:\n",
    "    a_df = pd.DataFrame(action_table([process_data[s]['worklist'], char_data[s]['worklist']], s, batch_id))\n",
    "    output_df = char_outputs(folder, s)\n",
    "    a_df = append_outputs(output_df, a_df)\n",
    "    a_df = a_df.astype({'chemical_from':'Int64'})\n",
    "    action_dfs.append(a_df)\n",
    "    fname = batch_id + '_' + s + '_action.csv'\n",
    "    fname = fname.replace(' ', '_')\n",
    "    a_df.to_csv(fname,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `link.csv` file\n",
    "## Helper functions for `link.csv`\n",
    "Links and link helper functions used to create the table with link information; used to link chemicals and nodes together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def goes_into_links(dissolve_rows):\n",
    "    \"\"\"\n",
    "    Helper method to create links for chemical nodes that \"go into\" a dissolve node.\n",
    "        AKA the \"GOES_INTO\" links\n",
    "    :param dissolve_rows: From an action table, takes in the rows of the table where dissolve\n",
    "        steps are involved. For this batch, this is at the beginning steps of the action table\n",
    "    \"\"\"\n",
    "    # step_id, action(link), chemical_from, step_to, chemical_to, step_from, sample_id, batch_id\n",
    "    row_template = [0, 'GOES_INTO', 0, 0, np.nan, np.nan]\n",
    "    links = []\n",
    "    for i in range(dissolve_rows.shape[0]):\n",
    "        row = dissolve_rows.iloc[i]\n",
    "        link = row_template.copy()\n",
    "        link[0] = row['step_id']\n",
    "        link[2] = row['chemical_from']\n",
    "        link[3] = link[2]\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def output_links(dissolve_rows):\n",
    "    \"\"\"\n",
    "    Helper method to create the OUTPUTS links from the initial solutes and solvents used\n",
    "    :param dissolve_rows: From an action table, takes in the rows of the table where dissolve\n",
    "        steps are involved. For this batch, this is at the beginning steps of the action table\n",
    "    \"\"\"\n",
    "    row_template = [0, 'OUTPUTS', np.nan, np.nan, 0, 0]\n",
    "    links = []\n",
    "    mix_step = dissolve_rows.iloc[-1]['step_id']+1\n",
    "    prev_step = dissolve_rows.iloc[-1]['step_id']\n",
    "    for i in range(dissolve_rows.shape[0]):\n",
    "        row = dissolve_rows.iloc[i]\n",
    "        link = row_template.copy()\n",
    "        link[0] = prev_step+1\n",
    "        prev_step +=1\n",
    "        link[4] = mix_step\n",
    "        link[5] = row['step_id']\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def next_func(other_steps, step_num):\n",
    "    # step_id, action, chemical_from, step_to, chemical_to, step_from\n",
    "    row_template = [0, 'NEXT', np.nan, np.nan, np.nan, 0]\n",
    "    rows = []\n",
    "    step_id = step_num\n",
    "    for i in range(other_steps.shape[0]):\n",
    "        step_to = other_steps.iloc[i]['step_id']\n",
    "        row = row_template.copy()\n",
    "        row[0] = step_id\n",
    "        row[3] = step_to\n",
    "        row[-1] = row[3] - 2\n",
    "        rows.append(row)\n",
    "        step_id += 2\n",
    "    return rows\n",
    "\n",
    "def char_link_helper(output_rows, char_nodes, step_id):\n",
    "    # step_id, action, chemical_from, step_to, chemical_to, step_from\n",
    "    row_template = [0, 'NEXT', np.nan, np.nan, np.nan, 0]\n",
    "    rows=[]\n",
    "\n",
    "    for i in range(char_nodes.shape[0]):\n",
    "        curr_output = output_rows.iloc[i]\n",
    "        char = curr_output['char_name']\n",
    "        if '_' in char:\n",
    "            char = 'plimaging'\n",
    "        curr_node = char_nodes[char_nodes['char_name']==char]\n",
    "        step_to = curr_output['step_id']\n",
    "        row = row_template.copy()\n",
    "        row[0] = step_id\n",
    "        row[3] = step_to\n",
    "        row[-1] = curr_node['step_id'].iloc[0]\n",
    "        rows.append(row)\n",
    "        step_id += 1\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing `link.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_table(action_table, sample_id, batch_id):\n",
    "    \n",
    "    link_cols = ['step_id',\n",
    "    'action',\n",
    "    'chemical_from',\n",
    "    'step_to',\n",
    "    'chemical_to',\n",
    "    'step_from',\n",
    "    'sample_id',\n",
    "    'batch_id']\n",
    "    \n",
    "    dissolve_rows = action_table[action_table['action'] == 'dissolve']\n",
    "    go_into = goes_into_links(dissolve_rows)\n",
    "    outputs = output_links(dissolve_rows)\n",
    "    \n",
    "    next_link_step = outputs[-1][0] + 3\n",
    "    \n",
    "    mix1_into_drop = [next_link_step, 'GOES_INTO', outputs[-1][4], outputs[-1][0]+1, np.nan, np.nan]\n",
    "    \n",
    "    next_link_step += 1 \n",
    "    \n",
    "    first_drop_step_id = action_table[action_table['action']=='drop'].iloc[0]['step_id']\n",
    "    second_drop_step_id = first_drop_step_id + 1\n",
    "    mix2_chem_id = mix1_into_drop[2] + 2\n",
    "    \n",
    "    mix1_to_mix2 = [next_link_step, 'NEXT', np.nan, np.nan, mix2_chem_id, first_drop_step_id]\n",
    "    \n",
    "    mix_antisolvent = [action_table.iloc[-1]['step_id']+4, 'GOES_INTO', mix2_chem_id-1, second_drop_step_id, np.nan, np.nan]\n",
    "    \n",
    "    next_link_step += 2\n",
    "    drop_to_mix2 = [next_link_step, 'NEXT', np.nan, np.nan, mix2_chem_id, second_drop_step_id]\n",
    "    \n",
    "    spin_step_id = action_table[action_table['action']=='spin'].iloc[0]['step_id']\n",
    "    mix2_to_spin = [next_link_step, 'GOES_INTO', mix2_chem_id, spin_step_id, np.nan, np.nan]\n",
    "    \n",
    "    if 'fid' in action_table.columns:\n",
    "        next_steps = action_table.loc[(action_table['step_id']>spin_step_id) & (pd.isna(action_table['fid']))]\n",
    "    else:\n",
    "        next_steps = action_table.loc[(action_table['step_id']>spin_step_id)]\n",
    "        \n",
    "    next_rows = next_func(next_steps, next_link_step+2)\n",
    "    \n",
    "    # for characterization outputs\n",
    "    if 'fid' in action_table.columns:\n",
    "        output_rows = action_table.loc[~pd.isna(action_table['fid'])]\n",
    "        char_rows = action_table.loc[(action_table['action'] == 'char') & (pd.isna(action_table['fid']))]\n",
    "        char_output_links = char_link_helper(output_rows, char_rows, next_rows[-1][0]+2)\n",
    "        \n",
    "        res = go_into + outputs + [mix1_into_drop] + [mix1_to_mix2] + [mix_antisolvent] + [drop_to_mix2] + [mix2_to_spin] + next_rows + char_output_links \n",
    "\n",
    "    else:\n",
    "        res = go_into + outputs + [mix1_into_drop] + [mix1_to_mix2] + [mix_antisolvent] + [drop_to_mix2] + [mix2_to_spin] + next_rows\n",
    "    \n",
    "    for i in res:\n",
    "        i.append(sample_id)\n",
    "        i.append(batch_id)\n",
    "        \n",
    "    return pd.DataFrame(res, columns=link_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving `link.csv` file\n",
    "#### 'batch_id' and 'folder' here can be changed to the necessary batch # + folder where characterization outputs are stored.\n",
    "* currently the batch id and folder corresponds to our test batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "# run to save all as csvs\n",
    "link_dfs = []\n",
    "batch_id = 'b19'\n",
    "# folder = 'Characterization_B19'\n",
    "\n",
    "for act in action_dfs:\n",
    "    l_df = link_table(act, act.iloc[0]['sample_id'], batch_id)\n",
    "    l_df = l_df.astype({'chemical_from':'Int64', 'step_to':'Int64', 'chemical_to':'Int64', 'step_from':'Int64'})\n",
    "    link_dfs.append(l_df)\n",
    "    fname = batch_id + '_' + act.iloc[0]['sample_id'] + '_link.csv'\n",
    "    l_df.to_csv(fname, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating `chem.csv` file\n",
    "## Helper functions for `chem.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chemicals(input_string):\n",
    "    result = []\n",
    "    for i in input_string.split('_'):\n",
    "        first_digit = search(r'\\d+.\\d+', i).start()\n",
    "        result.append((i[:first_digit], float(i[first_digit:])))\n",
    "    return result\n",
    "\n",
    "def create_new_row(**kwargs):\n",
    "    return dict(zip(kwargs, kwargs.values()))\n",
    "\n",
    "def check_name_format(chemicals_string_list):\n",
    "    \"\"\"Function to check if the chemicals from drop step follows \n",
    "    the format 'First0.75_Second0.10_Third0.5_Fourth0.5' \n",
    "    return True if yes and False if no\n",
    "    \"\"\"\n",
    "    return search(r'[A-Za-z]+\\d+.?\\d+_', chemicals_string_list) != None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing `chem.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chem_table(sample, batch_id):\n",
    "    chem_cols = ['chemical_id', 'batch_id', 'content', 'concentration', 'molarity', 'volume', 'chem_type']\n",
    "    chem = pd.DataFrame(columns = chem_cols)\n",
    "    \n",
    "    # the first chemical has the id of 1, the first mix has the id of 1\n",
    "    chemical_id = 1\n",
    "    mix_id = 1\n",
    "    \n",
    "    sample_id = sample['name']\n",
    "    worklist = sample['worklist']\n",
    "    for step in worklist:\n",
    "        # check if the 'details' is a key in worklist and check if steps is in the drops\n",
    "        # if yes, go check the content for the chemical\n",
    "        # if not, then move on since there is no chemical for mixing\n",
    "        if 'details' in step and 'drops' in step['details']:\n",
    "            for droplet in step['details']['drops']:\n",
    "                # if it has both solvent and solute\n",
    "                if 'solution' in droplet and droplet['solution']['solutes'] != '' and droplet['solution']['solvent'] != '':\n",
    "                    # check if the solutes string follows the format to further breaking \n",
    "                    # it down using check_name_format helper function\n",
    "                    # if yes, break the string down using the split_chemicals to get the name and the concentration\n",
    "                    if check_name_format(droplet['solution']['solutes']):\n",
    "                        for solute in split_chemicals(droplet['solution']['solutes']):\n",
    "                            content, concentration = solute\n",
    "                            if ((chem['content'] == content) & (chem['concentration'] == concentration)).sum() == 0:\n",
    "                                new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                         content = content, concentration = concentration, \n",
    "                                                         chem_type = 'solute', sample_id = sample_id)\n",
    "\n",
    "                                chem = chem.append(new_row, ignore_index=True)\n",
    "                                chemical_id += 1\n",
    "                    # if no, use the solute recipe name as the content (such as 'Xu-Recipe-PSK')\n",
    "                    else:\n",
    "                            new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                     content = droplet['solution']['solutes'], \n",
    "                                                     chem_type = 'solute', sample_id = sample_id)\n",
    "\n",
    "                            chem = chem.append(new_row, ignore_index=True)\n",
    "                            chemical_id += 1\n",
    "                    # check if the antisolvent string follows the format to further breaking \n",
    "                    # it down using check_name_format helper function\n",
    "                    # if yes, break the string down using the split_chemicals to get the name and the concentration\n",
    "                    if check_name_format(droplet['solution']['solvent']):\n",
    "                        for solvent in split_chemicals(droplet['solution']['solvent']):\n",
    "                            content, concentration = solvent\n",
    "                            if ((chem['content'] == content) & (chem['concentration'] == concentration)).sum() == 0:\n",
    "                                new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                         content = content, concentration = concentration, \n",
    "                                                         chem_type = 'solvent', sample_id = sample_id)\n",
    "                                chem = chem.append(new_row, ignore_index=True)\n",
    "                                chemical_id += 1\n",
    "                    # if no, use the solute recipe name as the content (such as 'Xu-Recipe-PSK')\n",
    "                    else:\n",
    "                        new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                 content = droplet['solution']['solutes'], \n",
    "                                                 chem_type = 'solvent', sample_id = sample_id)\n",
    "\n",
    "                        chem = chem.append(new_row, ignore_index=True)\n",
    "                        chemical_id += 1\n",
    "                    # adding the mix (or solution) from the previous solvents and solutes\n",
    "                    new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                             content = 'Mix'+str(mix_id), volume = droplet['volume'], \n",
    "                                             molarity = droplet['solution']['molarity'],\n",
    "                                             chem_type = 'solution', sample_id = sample_id)\n",
    "\n",
    "                    chem = chem.append(new_row, ignore_index=True)\n",
    "                    mix_id += 1\n",
    "                    chemical_id += 1\n",
    "                # check if the drop is an antisolvent (no solvent and no solute present)\n",
    "                if 'solution' in droplet and droplet['solution']['solutes'] == '':\n",
    "                    # check if the antisolvent is already in the df, add to the df if not in the df\n",
    "                    if (chem['content'] == droplet['solution']['solvent']).sum() == 0:\n",
    "                        new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                 content = droplet['solution']['solvent'],\n",
    "                                                 molarity = droplet['solution']['molarity'],\n",
    "                                                 chem_type = 'antisolvent', sample_id = sample_id)\n",
    "                        chem = chem.append(new_row, ignore_index=True)\n",
    "                        chemical_id += 1\n",
    "                        \n",
    "                    # adding the mix\n",
    "                    new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                             content = 'Mix'+str(mix_id),\n",
    "                                             volume = droplet['volume'],\n",
    "                                             chem_type = 'solution', sample_id = sample_id)\n",
    "\n",
    "                    chem = chem.append(new_row, ignore_index=True)\n",
    "                    mix_id += 1\n",
    "                    chemical_id += 1\n",
    "    return chem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving `chem.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chem_csv(samples, batch_id):\n",
    "    \"\"\"Takes in the dictionary of samples and run chem_table and save the resulting csv files.\n",
    "    Replaces the whitespace in the name with underscore for Neo4J compatibility\"\"\"\n",
    "    for sample in samples:\n",
    "        filename = batch_id + '_' + sample + '_' + 'chem.csv'\n",
    "        filename = filename.replace(' ', '_')\n",
    "        chem_table(samples[sample], batch_id).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_id = 'b19'\n",
    "save_chem_csv(process_data, batch_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Generator\n",
    "After having all the necessary `.csv` files in the current directory, we use the script below to make graph on Neo4j.\n",
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_columns(csv_file):\n",
    "    \"\"\"\n",
    "    Helper method to find all the columns\n",
    "    \n",
    "    :param csv_file: Takes in the csv file string, e.g:\n",
    "        'WBG Repeat, Batch 4 (Experiment 1)_sample12_action.csv'\n",
    "        \n",
    "    :return: Returns a list all the columns\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df.columns.to_list()\n",
    "\n",
    "def find_local_csv_files():\n",
    "    \"\"\"Find all the csv files ending with the name sample_chem.csv, sample_link.csv, \n",
    "    sample_action.csv within the current directory and group by sample\"\"\"\n",
    "    csv_files = [x for x in listdir() if x.endswith('.csv')]\n",
    "    unique_samples = list(set(findall('_(sample\\d+)_', ' '.join(csv_files))))\n",
    "\n",
    "    file_dict = {}\n",
    "    file_list = []\n",
    "\n",
    "    for file in csv_files:   \n",
    "        for sample in unique_samples:\n",
    "            if sample not in file_dict:\n",
    "                file_dict[sample] = {}\n",
    "\n",
    "            if '_{}_chem.csv'.format(sample) in file:\n",
    "                file_dict[sample]['chem'] = file\n",
    "                file_list.append(file)\n",
    "            elif '_{}_action.csv'.format(sample) in file:\n",
    "                file_dict[sample]['action'] = file\n",
    "                file_list.append(file)\n",
    "            elif '_{}_link.csv'.format(sample) in file:\n",
    "                file_dict[sample]['link'] = file\n",
    "                file_list.append(file)\n",
    "        \n",
    "    return file_dict, file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Action and Chem nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(filepath, node_type, cols, stored_folder = ''):\n",
    "    query = \"LOAD CSV WITH HEADERS FROM \\\"file:///\"\n",
    "    query += stored_folder\n",
    "    \n",
    "    if stored_folder != '':\n",
    "        query += '/'\n",
    "        \n",
    "    query += \"{}\\\" \".format(filepath)\n",
    "    if node_type == 'chem':\n",
    "        query += \"AS row CREATE (c:Chemical {\"\n",
    "    elif node_type == 'action':\n",
    "        query += \"AS row CREATE (a:Action {\"\n",
    "    \n",
    "    # add the columns into the query\n",
    "    query_columns = ([\"{}: row['{}'], \".format(cols[i], cols[i]) if i < len(cols) - 1 \n",
    "           else \"{}: row['{}']\".format(cols[i], cols[i]) for i in range(len(cols))])\n",
    "    query += ''.join(query_columns)\n",
    "    \n",
    "    # close the brackets and return\n",
    "    return query + \"});\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_links(filepath, stored_folder = ''):\n",
    "    query_start = \"LOAD CSV WITH HEADERS FROM \\\"file:///\"\n",
    "    query_start += stored_folder\n",
    "    \n",
    "    if stored_folder != '':\n",
    "        query_start += '/'\n",
    "    \n",
    "    query_start += \"{}\\\" \".format(filepath)\n",
    "    \n",
    "    query_1 = \"AS row MATCH (c:Chemical {chemical_id: row['chemical_from'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}), (a:Action {step_id:row['step_to'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}) CREATE (c)-[:GOES_INTO]->(a);\"\n",
    "    \n",
    "    query_2 = \"AS row MATCH (a1:Action {action: 'dissolve', step_id: row['step_from'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}),(c1:Chemical {chemical_id:row['chemical_to'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}) CREATE (a1)-[:OUTPUTS]->(c1);\"\n",
    "    \n",
    "    query_3 = \"AS row MATCH (a3:Action {action:'drop',step_id: row['step_from'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}),(c4:Chemical {chemical_id:row['chemical_to'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}) CREATE (a3)-[:NEXT]->(c4);\"\n",
    "    \n",
    "    query_4 = \"AS row MATCH (a3:Action {step_id: row['step_from'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}),(a4:Action {step_id:row['step_to'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}) CREATE (a3)-[:NEXT]->(a4);\"\n",
    "    \n",
    "\n",
    "    queries = []\n",
    "    for i in [query_1, query_2, query_3, query_4]:\n",
    "        query_str = query_start + i\n",
    "        queries.append(query_str)\n",
    "    \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Action, Chem, and Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_maker(chem_filepath, action_filepath, link_fileid, stored_folder = ''):\n",
    "    \"\"\"\n",
    "    Calls the other query functions in this one function, given the necessary file ids\"\"\"\n",
    "    queries = []\n",
    "    \n",
    "    chem_cols = find_columns(chem_filepath)\n",
    "    queries.append(create_nodes(chem_filepath, 'chem', chem_cols, stored_folder))\n",
    "    \n",
    "    action_cols = find_columns(action_filepath)\n",
    "    queries.append(create_nodes(action_filepath, 'action', action_cols, stored_folder))\n",
    "    \n",
    "    queries = queries + create_links(link_fileid, stored_folder)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking applicable `.csv` files to make `.cypher` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict, file_list = find_local_csv_files()\n",
    "queries = []\n",
    "neo4j_stored_folder = 'batch_test'\n",
    "\n",
    "for sample in file_dict:\n",
    "    # the input file_id is in the order of chem.csv, action.csv, link.csv\n",
    "     queries.append(query_maker(file_dict[sample]['chem'], \n",
    "                                file_dict[sample]['action'], \n",
    "                                file_dict[sample]['link'], \n",
    "                                stored_folder = neo4j_stored_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file as .cyper file\n",
    "output = open('output.cypher', 'w')\n",
    "for q in queries:\n",
    "    for query in q:\n",
    "        output.write(query)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the applicable `.csv` file after making the `.cypher` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    remove(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
