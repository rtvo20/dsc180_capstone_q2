{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, remove\n",
    "from os.path import join\n",
    "from json import load\n",
    "from tifffile import imread,imwrite\n",
    "from re import search, findall\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Explanation\n",
    "## Documentation for Data Preprocessing\n",
    "This notebook consists of two separate processes: (1) creating the `.csv` files for the chemical nodes, action nodes and links and (2) creating the `.cypher` file to load the `.csv` files into Neo4j database. After creating the `.cypher` file, the script deletes all `.csv` files that were used for imports.\n",
    "1. The detailed documentation for step (1) is in the notebook `preprocess explanation 1.ipynb`.\n",
    "2. The detailed documentation for step (2) is in the notebook `preprocess explanation 2.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the DBMS storage folders\n",
    "1. `filepath_import` specifies the import folder inside the DMBS folder. We will need to save the `.csv` files inside this folder for the `.cypher` to load all the data into the nodes.\n",
    "2. `filepath_bin` specifies the bin folder inside the DBMS folder. When we start a Database on Neo4j Desktop, we need to put the `.cypher` file inside DMBS bin folder for the terminal to read the `cypher shell`.\n",
    "3. `filepath_all_batches` specifies the location of all `psk` and `char` folders with the JSON worklist and Characterization folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_import = \"relate-data\\\\dbmss\\\\dbms-ea5729e6-f244-4c0d-b059-176025203af9\\\\import\"\n",
    "filepath_bin = \"relate-data\\\\dbmss\\\\dbms-ea5729e6-f244-4c0d-b059-176025203af9\\\\bin\"\n",
    "filepath_all_batches = \"files\"\n",
    "version_control_file = 'version_control.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Creating `.csv` files\n",
    "## Creating `action.csv` files\n",
    "### Helper functions for `action.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def step_helper(worklists):\n",
    "    \"\"\"\n",
    "    Helper function to get the step names for each step in worklist(s).\n",
    "    Takes either a single worklist or a list of worklists in as input and returns\n",
    "    a list of lists, with each list of steps corresponding to an input worklist.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "        \n",
    "    # iterate through worklists and get steps e.g. destination, drops, etc\n",
    "    for w in worklists:\n",
    "        w_steps = []\n",
    "        for step in w:\n",
    "            w_steps.append(list(step['details'].keys())[0])\n",
    "        steps.append(w_steps)\n",
    "    \n",
    "    return steps\n",
    "\n",
    "def dissolve_helper(drop_list, step_num = None):\n",
    "    \"\"\"\n",
    "    Helper function that takes in a list of drop steps and .\n",
    "    \"\"\"\n",
    "    drop_steps = drop_list['details']['drops']\n",
    "    \n",
    "    if drop_steps[0]['solution']['solutes'] != '':\n",
    "        num_chems = len(drop_steps[0]['solution']['solutes'].split('_'))\n",
    "        num_chems += len(drop_steps[0]['solution']['solvent'].split('_'))\n",
    "    else:\n",
    "        num_chems = 1\n",
    "    actions = []\n",
    "    for i in range(1, num_chems+1):\n",
    "        row = {}\n",
    "        row['step_id'] = i\n",
    "        row['action'] = 'dissolve'\n",
    "        row['chemical_from'] = i\n",
    "        actions.append(row)\n",
    "    return actions    \n",
    "\n",
    "def drop_helper(drop_list, step_num):\n",
    "    \"\"\"\n",
    "    Helper function to process drop steps in a worklist\n",
    "    \"\"\"\n",
    "    drop_list = drop_list['details']['drops']\n",
    "    \n",
    "    # exclude volume and solution information, as this is handled when \n",
    "    # creating the chem csvs/nodes\n",
    "    exclude_attributes = ['volume', 'solution']\n",
    "    row_attributes = [i for i in drop_list[0].keys() if i not in exclude_attributes]\n",
    "    drop_rows = []\n",
    "    i = 1\n",
    "    for drop in drop_list:\n",
    "        row = {}\n",
    "        for j in row_attributes:\n",
    "            row['step_id'] = step_num * 2 + i\n",
    "            row['action'] = 'drop'\n",
    "            row['chemical_from'] = step_num + i\n",
    "            row[\"drop_\"+j] = drop.get(j)\n",
    "        i += 1\n",
    "        drop_rows.append(row)\n",
    "    return drop_rows\n",
    "\n",
    "def spin_helper(spin_list, step_num):\n",
    "    \"\"\"\n",
    "    Helper function to process spin steps in a worklist\n",
    "    \"\"\"\n",
    "    spin_rows = []\n",
    "    spin_details = spin_list['details']['steps']\n",
    "    for spin in spin_details:\n",
    "        row = {}\n",
    "        row['step_id'] = step_num\n",
    "        row['action'] = 'spin'\n",
    "        for i in spin:\n",
    "            row[\"spin_\"+i] = spin.get(i)\n",
    "        step_num += 2\n",
    "        \n",
    "        attributes = ['start', 'start_actual', 'finish_actual', 'liquidhandler_timings', 'spincoater_log']\n",
    "        for i in attributes:\n",
    "            if i in spin_list.keys():\n",
    "                if (i == 'spincoater_log') and ('rpm' in spin_list[i]):\n",
    "                    log_attr = spin_list[i].keys()\n",
    "                    for j in log_attr:\n",
    "                        row['spin_log_'+j] = spin_list[i][j]\n",
    "                else:\n",
    "                    row[i] = spin_list[i]\n",
    "        spin_rows.append(row)\n",
    "    return spin_rows\n",
    "\n",
    "def anneal_helper(anneal_list, step_num):\n",
    "    \"\"\"\n",
    "    Helper function to process anneal steps in a worklist\n",
    "    \"\"\"\n",
    "    row = []\n",
    "    anneal_info = anneal_list['details']\n",
    "    anneal_row = {'step_id':step_num, 'action': 'anneal'}\n",
    "    for i in anneal_info:\n",
    "        anneal_row['anneal_'+i] = anneal_info.get(i)\n",
    "        \n",
    "    attributes = [i for i in anneal_list.keys() if i not in ['precedent', 'id', 'details']]\n",
    "    for i in attributes:\n",
    "        anneal_row[i] = anneal_list[i]\n",
    "    row.append(anneal_row)\n",
    "    return row\n",
    "\n",
    "def rest_helper(rest_step, step_num):\n",
    "    \"\"\"\n",
    "    Helper function to process rest steps in a worklist\n",
    "    \"\"\"\n",
    "    rest_row = {'step_id': step_num, 'action':'rest'}\n",
    "    rest_row['rest_duration'] = rest_step['details']['duration']\n",
    "    \n",
    "    attributes = [i for i in rest_step.keys() if i not in ['precedent', 'id', 'details']]\n",
    "    for i in attributes:\n",
    "        rest_row[i] = rest_step[i]\n",
    "    return [rest_row]\n",
    "\n",
    "def char_helper(char_list, step_num):\n",
    "    \"\"\"\n",
    "    Helper function to process characterization steps in a worklist\n",
    "    \"\"\"\n",
    "    char_rows = []\n",
    "    char_details = char_list['details']['characterization_tasks']\n",
    "    for char in char_details:\n",
    "        char_params = [i for i in list(char['details'].keys())]\n",
    "        char_info = [i for i in char if i != 'details']\n",
    "        row = {\n",
    "            'step_id':step_num, \n",
    "            'action':'char'\n",
    "        }\n",
    "        for i in char_params:\n",
    "            row['char_'+i] = char['details'][i]\n",
    "        for i in char_info:\n",
    "            if type(char[i]) == str:\n",
    "                if i == 'name':\n",
    "                    row['char_'+i] = char[i].lower().split(\"_\")[0]\n",
    "                else:\n",
    "                    row['char_'+i] = char[i].lower()\n",
    "            else:\n",
    "                row['char_'+i] = char[i]\n",
    "                \n",
    "        attributes = [i for i in char_list.keys() if i not in ['precedent', 'id', 'details', 'name', 'sample', ]]\n",
    "        for i in attributes:\n",
    "            row[i] = char_list[i]\n",
    "        char_rows.append(row)\n",
    "        step_num += 2\n",
    "        \n",
    "    return char_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE: function map used in action_table to call the helper functions above\n",
    "func_map = {\n",
    "    \"dissolve\": dissolve_helper,\n",
    "    \"drops\": drop_helper,\n",
    "    \"spin\": spin_helper,\n",
    "    \"anneal\": anneal_helper,\n",
    "    \"duration\": rest_helper,\n",
    "    \"characterization_tasks\": char_helper\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing `action.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def action_table(worklists, sample_id=np.nan, batch_id=np.nan):\n",
    "    \"\"\"\n",
    "    The action_table function takes in one or more worklists. \n",
    "    If there are multiple worklists, the first worklist will be the\n",
    "    psk worklist and the second will be the char worklist.\n",
    "    \"\"\"\n",
    "    if type(worklists[0]) != list:\n",
    "        worklists = [worklists]\n",
    "        \n",
    "    # obtain steps from worklists\n",
    "    steps = step_helper(worklists)\n",
    "    \n",
    "    rows = []\n",
    "    step_num = 1\n",
    "    for i in range(len(worklists)):\n",
    "        curr_worklist = worklists[i]\n",
    "        curr_steplist = steps[i]\n",
    "        for j in range(len(curr_steplist)):\n",
    "            if curr_steplist[j] == 'destination':\n",
    "                continue\n",
    "            else:\n",
    "                if curr_steplist[j] == 'drops':\n",
    "                    rows = rows + func_map['dissolve'](curr_worklist[j], step_num)\n",
    "                    step_num = rows[-1]['step_id']\n",
    "                    rows = rows + func_map['drops'](curr_worklist[j], step_num)\n",
    "                    step_num = rows[-1]['step_id']+3\n",
    "                    rows = rows + func_map['spin'](curr_worklist[j], step_num)\n",
    "                    step_num = rows[-1]['step_id']+2\n",
    "                else:\n",
    "                    if curr_steplist[j] == 'duration' and curr_worklist[j]['name'] == 'anneal':\n",
    "                        rows = rows + func_map['anneal'](curr_worklist[j], step_num)\n",
    "                    else:\n",
    "                        rows = rows + func_map[curr_steplist[j]](curr_worklist[j], step_num)\n",
    "                    step_num = rows[-1]['step_id']+2\n",
    "    \n",
    "    res = pd.DataFrame(rows)\n",
    "    res['sample_id'] = [sample_id] * res.shape[0]\n",
    "    res['batch_id'] = [batch_id] * res.shape[0]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characterization Task\n",
    "Need this as an input for the `link.csv` file.\n",
    "### Formatting `.tif` images to a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function used in char_outputs.\n",
    "def load_image(fid):\n",
    "    img = imread(fid) * 64 * 255\n",
    "    img = img.astype(np.float32)\n",
    "    img = np.dot(img[...,:3], [0.2989, 0.5870, 0.1140]) #convert to single channel/greyscale??\n",
    "    return img.astype(int) #truncate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def char_outputs(folder, sample):\n",
    "    path = folder + '/' + sample + \"/characterization0\"\n",
    "    fids = [f for f in listdir(path)]\n",
    "    \n",
    "    data = []\n",
    "    ids = []\n",
    "    for fid in fids:\n",
    "        if '.tif' in fid:\n",
    "            data.append(load_image(path+\"/\"+fid))\n",
    "        elif '.csv' in fid:\n",
    "            data.append(pd.read_csv(path+\"/\"+fid).drop(0,axis=0).to_dict())\n",
    "        else:\n",
    "            print('haven\\'t had to deal w this filetype yet')\n",
    "\n",
    "        ids.append(fid.split('_', 1)[1].split('.')[0])    \n",
    "    df = pd.DataFrame({'join_on': ids, 'fid':fids, 'output':data})\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_outputs(output_df, action_df):\n",
    "    \"\"\"\n",
    "    Helper function to append the characterization outputs as nodes to the action table\n",
    "    \"\"\"\n",
    "    step_id = action_df.iloc[-1]['step_id']+2\n",
    "    \n",
    "    row_template = action_df.iloc[-1].copy()\n",
    "    row_template.loc[:]=np.nan\n",
    "    row_template['action'] = 'char_output'\n",
    "    row_template['sample_id'] = action_df['sample_id'].iloc[0]\n",
    "    row_template['batch_id'] = action_df['batch_id'].iloc[0]\n",
    "    \n",
    "    output_rows = []\n",
    "    for r in range(output_df.shape[0]):\n",
    "        output_row = output_df.iloc[r]\n",
    "        row = row_template.copy()\n",
    "        row['step_id'] = step_id\n",
    "        row['char_name'] = output_row['join_on']\n",
    "        row['fid'] = output_row['fid']\n",
    "        row['output'] = output_row['output']\n",
    "        output_rows.append(row)\n",
    "        step_id += 1\n",
    "        \n",
    "    action_df = action_df.append(output_rows)\n",
    "    \n",
    "    return action_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_metrics(action_df, metric_row):\n",
    "    \"\"\"\n",
    "    Helper function that appends the fitted characterization metrics \n",
    "    as a node to the end of each sample's graph.\n",
    "    \"\"\"\n",
    "    step_id = action_df.iloc[-1]['step_id']+2\n",
    "    \n",
    "    row_template = action_df.iloc[-1].copy()\n",
    "    row_template.loc[:]=np.nan\n",
    "    row_template['step_id'] = step_id\n",
    "    row_template['action'] = 'fitted_metrics'\n",
    "    row_template['sample_id'] = action_df['sample_id'].iloc[0]\n",
    "    row_template['batch_id'] = action_df['batch_id'].iloc[0]\n",
    "    \n",
    "    for col in metric_row:\n",
    "        if 'name' in col:\n",
    "            pass\n",
    "        else:\n",
    "            row_template[col] = metric_row[col].iloc[0]\n",
    "    \n",
    "    action_df = action_df.append(row_template)\n",
    "    \n",
    "    return action_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the `action.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in samples, a list of json objects. \n",
    "# samples[0] is the psk worklist, samples[1] is the characterization worklist, if it exists.\n",
    "def save_action_csv(samples, batch_id, filepath = '', char_data=None, metrics=None):\n",
    "    \"\"\"\n",
    "    Function to parse worklist(s) in order to create action nodes.\n",
    "    char_data and metrics are optional parameters that are passed into the\n",
    "    function when such data exists and will be added to the sample's graph of data.\n",
    "    \"\"\"\n",
    "    action_dfs = []\n",
    "    for sample in samples[0]:\n",
    "        if samples[1] != None:\n",
    "            a_df = pd.DataFrame(action_table([samples[0][sample]['worklist'],\n",
    "                                             samples[1][sample]['worklist']],\n",
    "                                            sample, batch_id))\n",
    "        else:\n",
    "            a_df = pd.DataFrame(action_table([samples[0][sample]['worklist']], sample, batch_id))\n",
    "            \n",
    "        if char_data != None:\n",
    "            folder = 'files/' + char_data + '/Characterization'\n",
    "            output_df = char_outputs(folder, sample)\n",
    "            a_df = append_outputs(output_df, a_df)\n",
    "            \n",
    "        if isinstance(metrics, pd.DataFrame):\n",
    "            a_df = append_metrics(a_df, metrics[metrics['name'] == sample])\n",
    "            \n",
    "        a_df = a_df.astype({'chemical_from':'Int64'})\n",
    "        action_dfs.append(a_df)\n",
    "        fname = batch_id + '_' + sample + '_action.csv'\n",
    "        fname = fname.replace(' ', '_')\n",
    "        a_df.to_csv(join(filepath,fname),index=False)\n",
    "    return action_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `link.csv` file\n",
    "### Helper functions for `link.csv`\n",
    "Links and link helper functions used to create the table with link information; used to link chemicals and nodes together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "def goes_into_links(dissolve_rows):\n",
    "    \"\"\"\n",
    "    Helper method to create links for chemical nodes that \"go into\" a dissolve node.\n",
    "        AKA the \"GOES_INTO\" links\n",
    "    :param dissolve_rows: From an action table, takes in the rows of the table where dissolve\n",
    "        steps are involved. For this batch, this is at the beginning steps of the action table\n",
    "    \"\"\"\n",
    "    # step_id, action(link), chemical_from, step_to, chemical_to, step_from, sample_id, batch_id\n",
    "    row_template = [0, 'GOES_INTO', 0, 0, np.nan, np.nan]\n",
    "    links = []\n",
    "    for i in range(dissolve_rows.shape[0]):\n",
    "        row = dissolve_rows.iloc[i]\n",
    "        link = row_template.copy()\n",
    "        link[0] = row['step_id']\n",
    "        link[2] = row['chemical_from']\n",
    "        link[3] = link[2]\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def output_links(dissolve_rows):\n",
    "    \"\"\"\n",
    "    Helper method to create the OUTPUTS links from the initial solutes and solvents used\n",
    "    :param dissolve_rows: From an action table, takes in the rows of the table where dissolve\n",
    "        steps are involved. For this batch, this is at the beginning steps of the action table\n",
    "    \"\"\"\n",
    "    row_template = [0, 'OUTPUTS', np.nan, np.nan, 0, 0]\n",
    "    links = []\n",
    "    mix_step = dissolve_rows.iloc[-1]['step_id']+1\n",
    "    prev_step = dissolve_rows.iloc[-1]['step_id']\n",
    "    for i in range(dissolve_rows.shape[0]):\n",
    "        row = dissolve_rows.iloc[i]\n",
    "        link = row_template.copy()\n",
    "        link[0] = prev_step+1\n",
    "        prev_step +=1\n",
    "        link[4] = mix_step\n",
    "        link[5] = row['step_id']\n",
    "        links.append(link)\n",
    "    return links\n",
    "\n",
    "def next_func(other_steps, step_num):\n",
    "    \"\"\"\n",
    "    Helper method to create NEXT links. These links are pretty generic and indicate either:\n",
    "    1. Movement from one step to another\n",
    "    or\n",
    "    2. Movement from one task to the fitted metrics.\"\"\"\n",
    "    # step_id, action, chemical_from, step_to, chemical_to, step_from\n",
    "    row_template = [0, 'NEXT', np.nan, np.nan, np.nan, 0]\n",
    "    rows = []\n",
    "    step_id = step_num\n",
    "    if isinstance(other_steps, pd.Series):\n",
    "        step_to = other_steps['step_id']\n",
    "        row = row_template.copy()\n",
    "        row[0] = step_id\n",
    "        row[3] = step_to\n",
    "        row[-1] = row[3] - 2\n",
    "        rows.append(row)\n",
    "    else:\n",
    "        for i in range(other_steps.shape[0]):\n",
    "            step_to = other_steps.iloc[i]['step_id']\n",
    "            row = row_template.copy()\n",
    "            row[0] = step_id\n",
    "            row[3] = step_to\n",
    "            row[-1] = row[3] - 2\n",
    "            rows.append(row)\n",
    "            step_id += 2\n",
    "    return rows\n",
    "\n",
    "def char_link_helper(output_rows, char_nodes, step_id):\n",
    "    \"\"\"\n",
    "    Helper method to create NEXT links for characterization output nodes.\n",
    "    \"\"\"\n",
    "    # step_id, action, chemical_from, step_to, chemical_to, step_from\n",
    "    row_template = [0, 'NEXT', np.nan, np.nan, np.nan, 0]\n",
    "    rows=[]\n",
    "\n",
    "    for i in range(output_rows.shape[0]):\n",
    "        curr_output = output_rows.iloc[i]\n",
    "        char = curr_output['char_name']\n",
    "        if '_' in char:\n",
    "            char = 'plimaging'\n",
    "        curr_node = char_nodes[char_nodes['char_name']==char]\n",
    "        step_to = curr_output['step_id']\n",
    "        row = row_template.copy()\n",
    "        row[0] = step_id\n",
    "        row[3] = step_to\n",
    "        row[-1] = curr_node['step_id'].iloc[0]\n",
    "        rows.append(row)\n",
    "        step_id += 1\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing `link.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_table(action_table, sample_id, batch_id):\n",
    "    \"\"\"\n",
    "    Function that creates the table with link/relationship information between nodes.\n",
    "    \"\"\"\n",
    "    link_cols = ['step_id',\n",
    "    'action',\n",
    "    'chemical_from',\n",
    "    'step_to',\n",
    "    'chemical_to',\n",
    "    'step_from',\n",
    "    'sample_id',\n",
    "    'batch_id']\n",
    "    \n",
    "    # retrieve rows from the action_table where the action is dissolve\n",
    "    dissolve_rows = action_table[action_table['action'] == 'dissolve']\n",
    "    # create GOES_INTO links for the dissolve nodes, which link chemicals to dissolve nodes.\n",
    "    go_into = goes_into_links(dissolve_rows)\n",
    "    # create OUTPUTS links for the dissolve nodes, linking them to a chemical node (a new mix)\n",
    "    outputs = output_links(dissolve_rows)\n",
    "    \n",
    "    # variable used to track step_id for links.\n",
    "    next_link_step = outputs[-1][0] + 3\n",
    "    \n",
    "    # links solution from mix1 to the drop step\n",
    "    mix1_into_drop = [next_link_step, 'GOES_INTO', outputs[-1][4], outputs[-1][0]+1, np.nan, np.nan]\n",
    "    \n",
    "    # increment link step\n",
    "    next_link_step += 1 \n",
    "    \n",
    "    \n",
    "    first_drop_step_id = action_table[action_table['action']=='drop'].iloc[0]['step_id']\n",
    "    second_drop_step_id = first_drop_step_id + 1\n",
    "    mix2_chem_id = mix1_into_drop[2] + 2\n",
    "    \n",
    "    mix1_to_mix2 = [next_link_step, 'NEXT', np.nan, np.nan, mix2_chem_id, first_drop_step_id]\n",
    "    \n",
    "    mix_antisolvent = [action_table.iloc[-1]['step_id']+4, 'GOES_INTO', mix2_chem_id-1, second_drop_step_id, np.nan, np.nan]\n",
    "    \n",
    "    next_link_step += 2\n",
    "    drop_to_mix2 = [next_link_step, 'NEXT', np.nan, np.nan, mix2_chem_id, second_drop_step_id]\n",
    "    \n",
    "    spin_step_id = action_table[action_table['action']=='spin'].iloc[0]['step_id']\n",
    "    ######\n",
    "    next_link_step += 2\n",
    "    mix2_to_spin = [next_link_step, 'GOES_INTO', mix2_chem_id, spin_step_id, np.nan, np.nan]\n",
    "    \n",
    "    if 'fid' in action_table.columns:\n",
    "        next_steps = action_table.loc[(action_table['step_id']>spin_step_id) & (pd.isna(action_table['fid']))]\n",
    "    else:\n",
    "        next_steps = action_table.loc[(action_table['step_id']>spin_step_id)]\n",
    "        \n",
    "    next_rows = next_func(next_steps, next_link_step+2)\n",
    "    \n",
    "    # for characterization outputs\n",
    "    if 'fid' in action_table.columns:\n",
    "        output_rows = action_table.loc[~pd.isna(action_table['fid'])]\n",
    "        char_rows = action_table.loc[(action_table['action'] == 'char') & (pd.isna(action_table['fid']))]\n",
    "        char_output_links = char_link_helper(output_rows, char_rows, next_rows[-1][0]+2)\n",
    "        \n",
    "        res = go_into + outputs + [mix1_into_drop] + [mix1_to_mix2] + [mix_antisolvent] + [drop_to_mix2] + [mix2_to_spin] + next_rows + char_output_links \n",
    "\n",
    "    else:\n",
    "        res = go_into + outputs + [mix1_into_drop] + [mix1_to_mix2] + [mix_antisolvent] + [drop_to_mix2] + [mix2_to_spin] + next_rows\n",
    "        \n",
    "    for i in res:\n",
    "        i.append(sample_id)\n",
    "        i.append(batch_id)\n",
    "        \n",
    "    return pd.DataFrame(res, columns=link_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving `link.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_link_csv(samples, batch_id, filepath = ''):\n",
    "    link_dfs = []\n",
    "    for act in samples:\n",
    "        l_df = link_table(act, act.iloc[0]['sample_id'], batch_id)\n",
    "        l_df = l_df.astype({'chemical_from':'Int64', 'step_to':'Int64', 'chemical_to':'Int64', 'step_from':'Int64'})\n",
    "        link_dfs.append(l_df)\n",
    "        fname = batch_id + '_' + act.iloc[0]['sample_id'] + '_link.csv'\n",
    "        fname = fname.replace(' ', '_')\n",
    "        l_df.to_csv(join(filepath, fname), index=False)\n",
    "    return link_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating `chem.csv` file\n",
    "### Helper functions for `chem.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chemicals(input_string):\n",
    "    result = []\n",
    "    for i in input_string.split('_'):\n",
    "        first_digit = search(r'\\d+.\\d+', i).start()\n",
    "        result.append((i[:first_digit], float(i[first_digit:])))\n",
    "    return result\n",
    "\n",
    "def create_new_row(**kwargs):\n",
    "    return dict(zip(kwargs, kwargs.values()))\n",
    "\n",
    "def check_name_format(chemicals_string_list):\n",
    "    \"\"\"Function to check if the chemicals from drop step follows \n",
    "    the format 'First0.75_Second0.10_Third0.5_Fourth0.5' \n",
    "    return True if yes and False if no\n",
    "    \"\"\"\n",
    "    return search(r'[A-Za-z]+\\d+.?\\d+_', chemicals_string_list) != None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing `chem.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chem_table(sample, batch_id):\n",
    "    chem_cols = ['chemical_id', 'batch_id', 'content', 'concentration', 'molarity', 'volume', 'chem_type']\n",
    "    chem = pd.DataFrame(columns = chem_cols)\n",
    "    \n",
    "    # the first chemical has the id of 1, the first mix has the id of 1\n",
    "    chemical_id = 1\n",
    "    mix_id = 1\n",
    "    \n",
    "    sample_id = sample['name']\n",
    "    worklist = sample['worklist']\n",
    "    for step in worklist:\n",
    "        # check if the 'details' is a key in worklist and check if steps is in the drops\n",
    "        # if yes, go check the content for the chemical\n",
    "        # if not, then move on since there is no chemical for mixing\n",
    "        if 'details' in step and 'drops' in step['details']:\n",
    "            for droplet in step['details']['drops']:\n",
    "                # if it has both solvent and solute\n",
    "                if 'solution' in droplet and droplet['solution']['solutes'] != '' and droplet['solution']['solvent'] != '':\n",
    "                    # check if the solutes string follows the format to further breaking \n",
    "                    # it down using check_name_format helper function\n",
    "                    # if yes, break the string down using the split_chemicals to get the name and the concentration\n",
    "                    if check_name_format(droplet['solution']['solutes']):\n",
    "                        for solute in split_chemicals(droplet['solution']['solutes']):\n",
    "                            content, concentration = solute\n",
    "                            if ((chem['content'] == content) & (chem['concentration'] == concentration)).sum() == 0:\n",
    "                                new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                         content = content, concentration = concentration, \n",
    "                                                         chem_type = 'solute', sample_id = sample_id)\n",
    "\n",
    "                                chem = chem.append(new_row, ignore_index=True)\n",
    "                                chemical_id += 1\n",
    "                    # if no, use the solute recipe name as the content (such as 'Xu-Recipe-PSK')\n",
    "                    else:\n",
    "                            new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                     content = droplet['solution']['solutes'], \n",
    "                                                     chem_type = 'solute', sample_id = sample_id)\n",
    "\n",
    "                            chem = chem.append(new_row, ignore_index=True)\n",
    "                            chemical_id += 1\n",
    "                    # check if the antisolvent string follows the format to further breaking \n",
    "                    # it down using check_name_format helper function\n",
    "                    # if yes, break the string down using the split_chemicals to get the name and the concentration\n",
    "                    if check_name_format(droplet['solution']['solvent']):\n",
    "                        for solvent in split_chemicals(droplet['solution']['solvent']):\n",
    "                            content, concentration = solvent\n",
    "                            if ((chem['content'] == content) & (chem['concentration'] == concentration)).sum() == 0:\n",
    "                                new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                         content = content, concentration = concentration, \n",
    "                                                         chem_type = 'solvent', sample_id = sample_id)\n",
    "                                chem = chem.append(new_row, ignore_index=True)\n",
    "                                chemical_id += 1\n",
    "                    # if no, use the solute recipe name as the content (such as 'Xu-Recipe-PSK')\n",
    "                    else:\n",
    "                        new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                 content = droplet['solution']['solutes'], \n",
    "                                                 chem_type = 'solvent', sample_id = sample_id)\n",
    "\n",
    "                        chem = chem.append(new_row, ignore_index=True)\n",
    "                        chemical_id += 1\n",
    "                    # adding the mix (or solution) from the previous solvents and solutes\n",
    "                    new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                             content = 'Mix'+str(mix_id), volume = droplet['volume'], \n",
    "                                             molarity = droplet['solution']['molarity'],\n",
    "                                             chem_type = 'solution', sample_id = sample_id)\n",
    "\n",
    "                    chem = chem.append(new_row, ignore_index=True)\n",
    "                    mix_id += 1\n",
    "                    chemical_id += 1\n",
    "                # check if the drop is an antisolvent (no solvent and no solute present)\n",
    "                if 'solution' in droplet and droplet['solution']['solutes'] == '':\n",
    "                    # check if the antisolvent is already in the df, add to the df if not in the df\n",
    "                    if (chem['content'] == droplet['solution']['solvent']).sum() == 0:\n",
    "                        new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                                 content = droplet['solution']['solvent'],\n",
    "                                                 molarity = droplet['solution']['molarity'],\n",
    "                                                 chem_type = 'antisolvent', sample_id = sample_id)\n",
    "                        chem = chem.append(new_row, ignore_index=True)\n",
    "                        chemical_id += 1\n",
    "                        \n",
    "                    # adding the mix\n",
    "                    new_row = create_new_row(chemical_id = chemical_id, batch_id = batch_id, \n",
    "                                             content = 'Mix'+str(mix_id),\n",
    "                                             volume = droplet['volume'],\n",
    "                                             chem_type = 'solution', sample_id = sample_id)\n",
    "\n",
    "                    chem = chem.append(new_row, ignore_index=True)\n",
    "                    mix_id += 1\n",
    "                    chemical_id += 1\n",
    "    return chem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving `chem.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chem_csv(samples, batch_id, filepath = ''):\n",
    "    \"\"\"Takes in the dictionary of samples and run chem_table and save the resulting csv files.\n",
    "    Replaces the whitespace in the name with underscore for Neo4J compatibility\"\"\"\n",
    "    for sample in samples:\n",
    "        filename = batch_id + '_' + sample + '_' + 'chem.csv'\n",
    "        filename = filename.replace(' ', '_')\n",
    "        chem_table(samples[sample], batch_id).to_csv(join(filepath,filename), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving `.csv` files in Neo4j import folder\n",
    "### Helper functions to find the files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_psk_files(directory = 'files'):\n",
    "    \"\"\"Takes in the name of the folder with all of the psk batches.\n",
    "    Returns a list of tuples with the batch name and the maestro JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory: str\n",
    "        The name of the folder with all of the psk batches.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of tuples with first element being the batch name and the second element \n",
    "        being the full filepath of the maestro JSON file.\n",
    "        \n",
    "    Example\n",
    "    -------\n",
    "    >>> find_psk_json_files('files')\n",
    "    [('20230110_B18-psk', 'files\\\\20230110_B18-psk\\\\maestro_sample_log.json'),\n",
    "     ('20230114_B18-psk_1', 'files\\\\20230114_B18-psk_1\\\\maestro_sample_log.json')]\n",
    "    \"\"\"\n",
    "    json_files = []\n",
    "    # going to every folder to find all the psk folders with -psk suffix\n",
    "    for folder in listdir(directory):\n",
    "        if search(r'-psk(_\\d)*$', folder) != None:\n",
    "            # then check if the .json file is available in each of the psk folder\n",
    "            for file in listdir(join(directory, folder)):\n",
    "                if file.endswith('.json'):\n",
    "                    json_files.append((folder, join(directory, folder, file)))\n",
    "    return json_files\n",
    "\n",
    "def find_char_file(psk_batch, directory = 'files'):\n",
    "    try:\n",
    "        batch_char_folders = []\n",
    "        batch = search(r'(?:\\d+_)(\\w+)', psk_batch).group(1)\n",
    "        for char_folder_name in listdir(directory):\n",
    "            if batch + '-char' in char_folder_name:\n",
    "                if search(r'\\d+$', char_folder_name) != None:\n",
    "                    char_number = int(search(r'\\d+$', char_folder_name).group(0))\n",
    "                    batch_char_folders.append((char_number, char_folder_name))\n",
    "                else:\n",
    "                    batch_char_folders.append((0, char_folder_name))\n",
    "        return sorted(batch_char_folders, key=lambda x: x[0], reverse=True)[0][1]\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def find_fitted_char_metrics(psk_batch, directory = 'files'):\n",
    "    for file in listdir(join(directory, find_char_file(psk_batch, directory))):\n",
    "        if ('fitted_characterization' in file or 'characterization_metrics' in file) and '.csv' in file:\n",
    "            return join(directory, find_char_file(psk_batch, directory), file)\n",
    "    return None\n",
    "\n",
    "def find_new_batches(filepath_all_batches, version_control_file = 'version_control.csv'):\n",
    "    psk_folders_files = find_psk_files(filepath_all_batches)\n",
    "    all_batches = [i[0] for i in psk_folders_files]\n",
    "    current_batches = pd.read_csv(version_control_file)['current_batches'].to_list()\n",
    "    \n",
    "    new_batches = []\n",
    "    for folder, file in psk_folders_files:\n",
    "        if folder not in current_batches:\n",
    "            new_batches.append((folder, file))\n",
    "            \n",
    "    pd.DataFrame(all_batches, columns = ['current_batches']).to_csv(version_control_file, index=False)\n",
    "    return new_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('20221206_B17-psk', 'files\\\\20221206_B17-psk\\\\maestro_sample_log.json'),\n",
       " ('20230114_B18-psk_1', 'files\\\\20230114_B18-psk_1\\\\maestro_sample_log.json'),\n",
       " ('20230122_B19-psk', 'files\\\\20230122_B19-psk\\\\maestro_sample_log.json')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_psk_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving `.csv` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_batches = find_new_batches(filepath_all_batches, version_control_file)\n",
    "\n",
    "action_df_list = []\n",
    "for batch_id, filename in new_batches:\n",
    "    # handle char folder/files, if they exist\n",
    "    char_file = find_char_file(batch_id, filepath_all_batches)\n",
    "    char_data = None\n",
    "    if char_file != None:\n",
    "        f = open(join(filepath_all_batches, char_file, 'maestro_sample_log.json'))\n",
    "        char_data = load(f)\n",
    "        f = f.close()\n",
    "    \n",
    "    metrics_file = find_fitted_char_metrics(batch_id, filepath_all_batches)\n",
    "    metrics = None\n",
    "    if metrics_file != None:\n",
    "        metrics = pd.read_csv(metrics_file)\n",
    "    \n",
    "    f = open(filename)\n",
    "    data = load(f)\n",
    "    f = f.close()\n",
    "    action_df_list = save_action_csv([data, char_data], batch_id, filepath_import, char_file, metrics)\n",
    "    save_link_csv(action_df_list, batch_id, filepath_import)\n",
    "    save_chem_csv(data, batch_id, filepath_import)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Graph Generator\n",
    "After having all the necessary `.csv` files in the current directory, we use the script below to make graph on Neo4j.\n",
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_columns(csv_file):\n",
    "    \"\"\"\n",
    "    Helper method to find all the columns\n",
    "    \n",
    "    :param csv_file: Takes in the csv file string, e.g:\n",
    "        'WBG Repeat, Batch 4 (Experiment 1)_sample12_action.csv'\n",
    "        \n",
    "    :return: Returns a list all the columns\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df.columns.to_list()\n",
    "\n",
    "def find_csv_files(filepath):\n",
    "    \"\"\"Find all the csv files ending with the name sample_chem.csv, sample_link.csv, \n",
    "    sample_action.csv within the current directory and group by sample\"\"\"\n",
    "    csv_files = [x for x in listdir(filepath) if x.endswith('.csv')]\n",
    "    unique_samples = list(set(findall('([\\w-]+sample\\d+)', ' '.join(csv_files))))\n",
    "\n",
    "    file_dict = {}\n",
    "    file_list = []\n",
    "    \n",
    "    types = ['action', 'chem', 'link']\n",
    "\n",
    "    for sample in unique_samples:\n",
    "        csv_tables = [sample + '_' + i + '.csv' for i in types]\n",
    "        file_list += csv_tables\n",
    "        file_dict[sample] = dict(zip(types, csv_tables))\n",
    "        \n",
    "    return file_dict, file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Action and Chem nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_node(filepath, node_type, cols, stored_folder = ''):\n",
    "    query = \"LOAD CSV WITH HEADERS FROM \\\"file:///\"\n",
    "    query += stored_folder\n",
    "    \n",
    "    if stored_folder != '':\n",
    "        query += '/'\n",
    "        \n",
    "    filepath = filepath.split('\\\\')[-1]\n",
    "    query += \"{}\\\" \".format(filepath)\n",
    "    if node_type == 'chem':\n",
    "        query += \"AS row CREATE (c:Chemical {\"\n",
    "    elif node_type == 'action':\n",
    "        query += \"AS row CREATE (a:Action {\"\n",
    "    \n",
    "    # add the columns into the query\n",
    "    query_columns = ([\"{}: row['{}'], \".format(cols[i], cols[i]) if i < len(cols) - 1 \n",
    "           else \"{}: row['{}']\".format(cols[i], cols[i]) for i in range(len(cols))])\n",
    "    query += ''.join(query_columns)\n",
    "    \n",
    "    # close the brackets and return\n",
    "    return query + \"});\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_links(filepath, stored_folder = ''):\n",
    "    query_start = \"LOAD CSV WITH HEADERS FROM \\\"file:///\"\n",
    "    query_start += stored_folder\n",
    "    \n",
    "    if stored_folder != '':\n",
    "        query_start += '/'\n",
    "    \n",
    "    filepath = filepath.split('\\\\')[-1]\n",
    "    query_start += \"{}\\\" \".format(filepath)\n",
    "    \n",
    "    query_1 = \"AS row MATCH (c:Chemical {chemical_id: row['chemical_from'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}), (a:Action {step_id:row['step_to'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}) CREATE (c)-[:GOES_INTO]->(a);\"\n",
    "    \n",
    "    query_2 = \"AS row MATCH (a1:Action {action: 'dissolve', step_id: row['step_from'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}),(c1:Chemical {chemical_id:row['chemical_to'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}) CREATE (a1)-[:OUTPUTS]->(c1);\"\n",
    "    \n",
    "    query_3 = \"AS row MATCH (a3:Action {action:'drop',step_id: row['step_from'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}),(c4:Chemical {chemical_id:row['chemical_to'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}) CREATE (a3)-[:NEXT]->(c4);\"\n",
    "    \n",
    "    query_4 = \"AS row MATCH (a3:Action {step_id: row['step_from'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}),(a4:Action {step_id:row['step_to'], sample_id: row['sample_id'], \\\n",
    "batch_id: row['batch_id']}) CREATE (a3)-[:NEXT]->(a4);\"\n",
    "    \n",
    "\n",
    "    queries = []\n",
    "    for i in [query_1, query_2, query_3, query_4]:\n",
    "        query_str = query_start + i\n",
    "        queries.append(query_str)\n",
    "    \n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Action, Chem, and Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_maker(chem_filepath, action_filepath, link_fileid, stored_folder = ''):\n",
    "    \"\"\"\n",
    "    Calls the other query functions in this one function, given the necessary file ids\"\"\"\n",
    "    queries = []\n",
    "    \n",
    "    chem_cols = find_columns(chem_filepath)\n",
    "    queries.append(create_node(chem_filepath, 'chem', chem_cols, stored_folder))\n",
    "    \n",
    "    action_cols = find_columns(action_filepath)\n",
    "    queries.append(create_node(action_filepath, 'action', action_cols, stored_folder))\n",
    "    \n",
    "    queries = queries + create_links(link_fileid, stored_folder)\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking applicable `.csv` files to make `.cypher` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dict, file_list = find_csv_files(filepath_import)\n",
    "queries = []\n",
    "\n",
    "for sample in file_dict:\n",
    "    # the input file_id is in the order of chem.csv, action.csv, link.csv\n",
    "    queries.append(query_maker(join(filepath_import, file_dict[sample]['chem']), \n",
    "                                join(filepath_import, file_dict[sample]['action']), \n",
    "                                join(filepath_import, file_dict[sample]['link']), \n",
    "                                stored_folder = '')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file as .cypher file\n",
    "output = open(join(filepath_bin,'output.cypher'), 'w')\n",
    "for q in queries:\n",
    "    for query in q:\n",
    "        output.write(query)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleting the applicable `.csv` file after making the `.cypher` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_list:\n",
    "    remove(join(filepath_import, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
