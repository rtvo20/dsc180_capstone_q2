# DSC 180 Capstone Project

*Most of this description was from the README/documentation located on our previous repo, with some slight changes. The repo can be found here* [here](https://github.com/rtvo20/dsc180_quarter1_submission).

This project uses data from David Fenning's Solar Energy Innovation Laboratory (SOLEIL) that creates solar cell samples. The data they collect are information from the manufacturing process of these solar cells, along with data they collect when testing the samples. Our project cleans and transforms the data, which are originally in JSON format, before saving them as CSVs that allow it to be imported and graphed in Neo4j, a Graph DBMS. The purpose of this task is to have a pipeline that can organize and transform the data so that it can be graphed in Neo4j and can be queried.

Input data are JSON files containing information from the SOLEIL lab in the form of a worklist. Our functions extract data from these worklists, such as step names (e.g. 'drop', 'spin', 'hotplate'), chemical names, and output data (measurements and tests done on the resulting sample). Running run.py on the input data gives us the output data file, which can be used to generate our output, a graph representation of the data.

The output data (CSVs) after they are cleaned and transformed are run through a function that generates a script file with queries in Neo4j's query language, Cypher. The script (a .cypher file) automates the process to graph the data using Neo4j's Cypher shell terminal. To run the script, we use a Docker container running Neo4j, which requires us to copy this 'output.cypher' file into the docker container's root directory before we can run it with a command. All of the instructions to do so are located in the section below.

This current iteration includes test data under test/testdata, which is one sample from some actual data to show how our code works on "barebones" test data.

More detailed instructions are below, however an overview of the process to reproduce our results:

run python run.py test to generate a script file with queries to generate a graph's nodes and links.
Using Docker, pull the latest Neo4j Docker image and start a container with this image.
Copy the script file from DSMLP to the local setting, then copy it to the docker container's root directory
Open a terminal in the docker container and run the script file and produce the results.

More detailed instructions are below, however an overview of the process to reproduce our results:
1. run ```python run.py test``` to generate a script file with queries to generate a graph's nodes and links.
2. Using Docker, pull the latest Neo4j Docker image and start a container with this image.
   * Copy the script file from DSMLP to the local setting, then copy it to the docker container's root directory
   * Open a terminal in the docker container and run the script file and produce the results.

## To run the project use run.py and follow the instructions below.

* The filepaths to the test data are already coded into ```run.py``` and are under the folder "test/testdata".
* The available targets for running ```python run.py <target>``` and the order of the targets are:
    * ```data```>```features```>```queries```
    * Alternatively, running the command ```python run.py test``` is equivalent to running each of the above targets sequentially.
* Running ```run.py``` cleans and transforms the data and creates queries in Neo4j's query language (Cypher) that allows for nodes and links to be graphed. Each graph in our implementation currently requires 6 queries to create and link all the nodes, so to help automate the process, the output of ```run.py``` is a Neo4j script-type file (.cypher file) that performs all of these queries in less inputs than doing so manually.
  * Our output file is named "output.cypher" and will be located in the project's root directory.

## To run the script generated by the run.py script above, use Docker

* The following docker run command sets up a docker container with all of the necessary flags and config settings. The command is all one line, it should be copied and pasted in its entirety in a local terminal.
    * ```docker run -p 7474:7474 -p 7687:7687 -v $PWD/data:/data -v $PWD/plugins:/plugins --name neo4j-apoc -e NEO4J_apoc_export_file_enabled=true -e NEO4J_apoc_import_file_enabled=true -e NEO4J_apoc_import_file_use__neo4j__config=true -e NEO4JLABS_PLUGINS=\[\"apoc\"\] neo4j:4.0```
    * In short, the flags set up permissions that allow for moving files between the Docker container's storage volume and local storage. It also enables for usage of APOC, a Neo4j package used to help export queried data.
    * Wait for the Docker container to initialize and start up, and in an internet browser navigate to `localhost:7474`.
    * This opens up Neo4j's browser UI and upon accessing it for the first time, should prompt the user to create a username/password; although it is possible to set it up with no authentication required under the "Authentication type" drop-down menu.
    * The default username/password is neo4j/neo4j. Once entered, it will then ask for a new password.

* The next steps require copying the output of the `run.py` file, "output.cypher" and the output CSVs into the Docker container's directory.
    * If the output.cypher file is located on DSMLP, it should be downloaded/copied to a local directory first.
    * In a local terminal, change directory to the location of the cypher file and CSVs, then run the following command
      * ```docker cp output.cypher neo4j-apoc:/var/lib/neo4j/import/output.cypher```
    * Then, perform the same process and copy the CSVs into the Docker container.
      * ```docker cp b19_sample0_chem.csv neo4j-apoc:/var/lib/neo4j/import/```
      * ```docker cp b19_sample0_action.csv neo4j-apoc:/var/lib/neo4j/import/```
      * ```docker cp b19_sample0_link.csv neo4j-apoc:/var/lib/neo4j/import/```

* With the necessary files copied over to the docker container, go to the docker container and select "Open in Terminal", as seen in the image below. 

![image](https://user-images.githubusercontent.com/59627502/218381794-04ed9f95-5fc9-4102-aa9c-d5c87adcee41.png)

  * In this docker terminal, `cd import`
  * and again, in the docker terminal, run the following command:
    * `cypher-shell -f output.cypher -u neo4j -p test` (replacing your username and password where `neo4j` and `test` are respectively.  
  * Back in the browser at `localhost:7474`, `MATCH(n) RETURN n` can be entered in the query field and run to return the nodes and relationships graphed by our output.

![image](https://user-images.githubusercontent.com/59627502/218383326-7880d998-0aeb-4b63-8ce6-24aad0ae5f85.png)
* This is our result from a graph containing multiple samples, but the test data will contain just 1 sample.

## Additional features
At the final stage of our project, we moved from using Neo4j Docker to Neo4j Desktop, as this version has more practical use for the lab team. Our run.py and this README file contain instructions and functionality for Neo4j Docker in order to support reproducibility; however we have included a `preprocessing.ipynb` that contains several features used with Neo4j Desktop that is useful in practice for the lab team. 
To briefly describe those features:
  * Saving `action`, `chem`, and `link` CSV files along with cypher files to the appropriate folders within the local Neo4j Desktop installation
    * CSV files are saved to the `import` folder within the Neo4j Desktop database directory
    * Cypher files are saved to the `bin` folder within the Neo4j Desktop database directory 
  * Version control to keep track of batches of data that have already been processed, to avoid unnecessary redundant processing of batchs.
  * Deleting CSV files after they are used to load data into Neo4j to reduce clutter in file storage.
